---
title: "Presidential excitable speech analysis (1)"
author: "joahn"
date: '2020-06-05'
slug: presidential-excitable-speech-analysis_1
output: html_document
tags: []
categories: []
---
> 
본 프로젝트는 2회에 걸쳐 게시되었습니다. 첫 번째 게시물은 자료 수집과 전처리 및 산출식 개발 등에 초점을 맞추고, 두 번째 게시물은 본격적인 상관분석, 시각화와 주관적 분석을 다루고 있습니다.  

* * *

목차

1. 도입
    1. 간단한 소개
    2. 국정 지지도 자료 수집
2. 문재앙 지수 만들기
    1. 댓글 크롤링
        1. 네이버
        2. 다음
    2. 국정 지지도와 초벌 상관 분석
    3. 인사이트 파악 및 산출식 개발

```{r setup, include=FALSE}
setwd("C:/Users/Byeongjun Cho/Documents/blog/blog/content/posts")
library(tidyverse)
library(lubridate)
library(plyr)
library(readxl)
library(writexl)
library(tictoc)
library(N2H4)
library(xml2)
library(XML)
library(rvest)
library(broom)
library(ggplot2)
```

## 간단한 소개

<strong> 2018.2 ~ 문재인 현 대통령의 국정 지지도</strong>와 네이버/다음 메인 시사 뉴스 댓글 중 <strong>'문재앙' 키워드를 포함한 댓글의 좋아요/싫어요 수치</strong>를 구한다. 이를 상관 분석을 통해 비교하고자 한다.

## 국정 지지도 자료 수집
 
 현 대통령의 국정 지지도 수치는 국내 여론조사 기관 '한국갤럽'과 '리얼미터'의 주간 집계자료를 이용하되, 위키백과 항목에 주간별로 정리된 테이블을 스프레드시트에 옮겨 R로 전처리 후 사용한다. 보다 세부적인 대통령 국정평가 항목, 즉 '긍정평가' '부정평가' 수치는 각 여론조사 기관 홈페이지에 게재되는 주간 동향 보도자료 내 수치를 스프레드시트로 정리한 후 사용한다.
 cf. 구글 스프레드시트를 통해 크롤링하는 방법도 고려했지만, 여론조사 홈페이지 URL 링크의 불규칙성, 한글 주소의 인코딩 문제, PDF 파일 크롤링의 어려움 등을 감안해 주간 자료를 직접 table로 정리하는 것이 효율적이라고 판단했다.
 
 국정 지지도 자료를 구하는 데에는 크게 2가지 경로가 있다. 하나는 [위키백과 '대한민국의 대통령 지지율' 항목](https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EB%8C%80%ED%86%B5%EB%A0%B9_%EC%A7%80%EC%A7%80%EC%9C%A8)에서 표로 정리된 리얼미터/한국갤럽 자료를 이용하는 것이고, 다른 하나는 해당 조사기관 홈페이지에 주마다 게시되는 보도자료를 직접 참조하는 것이다. 나는 일차적인 분석을 위해 간편한 위키백과 도표 자료를 이용하고, 일정한 인사이트를 얻은 후 해당 홈페이지에 게시된 보도자료의 더 상세한 수치를 직접 구글스프레드 시트 등으로 정리하여 R로 불러왔다.

```{r poll, echo = TRUE, eval = TRUE}

# 조사기관 보도자료 내 긍정/부정평가 항목 데이터 불러오기 (리얼미터, 한국갤럽)

rlmeter_president = read_xlsx("poll_president.xlsx", sheet = 1, range = cell_cols("A:H"))
rlmeter_president
```
```{r poll2, echo = TRUE, eval = TRUE}
gallup_president = read_xlsx("poll_president.xlsx", sheet = 2, range = cell_cols("B:H"))
gallup_president = bind_cols(gallup_president[1:120,], rlmeter_president[,1]) %>% 
  select(week, everything(), -`월(M)`, -`주(W)`, -`표본 수`)
gallup_president
```
```{r echo=TRUE, eval=TRUE}

# 위키백과 '대통령 지지도' 항목에서 긁어온 데이터 전처리

wiki = read_xlsx("wiki_president.xlsx")
```
```{r, include=FALSE}
for (i in 39:71){
  wiki[i-1, 1:3] = wiki[i,1:3]}
for (i in 73:160){
  wiki[i-2, 1:3] = wiki[i,1:3]}

wiki = wiki[38:156, 1:3]
wiki = wiki %>% mutate(gallup = as.character(`한국갤럽[3]`),
                gallup = ifelse(gallup == "미조사", NA, round(as.numeric(gallup),4)),
                rlmeter = as.character(`리얼미터[4]`),
                rlmeter = ifelse(rlmeter == "미조사", NA, round(as.numeric(rlmeter),4)))
wiki$gallup = as.numeric(wiki$gallup)
wiki$rlmeter = as.numeric(wiki$rlmeter)
wiki = wiki %>%
  mutate(gallup = coalesce(gallup, 0), rlmeter = coalesce(rlmeter, 0))
wiki

```
```{r visual, echo=TRUE}
par(mfrow=c(2,1))
plot.ts(wiki$gallup)
plot.ts(wiki$rlmeter)
```

위키백과 항목의 결측값 탓에 푹 꺼진 부분을 제외하고 대강의 추세를 살펴보면, 지난 약 2년여간 대통령의 지지율은 꾸준히 감소해왔음을 알 수 있다. 이는 집권 중반기를 지난 탓에 통상적인 레임덕 현상이 나타났음으로 보이며, 다만 코로나 사태에 대해 성공적인 국정 운영으로 평가받는 최근에 들어서는 지지율이 다소 상승한 추세를 확인할 수 있다.

## 문재앙 지수 만들기 (1) 댓글 크롤링

## 네이버 기사 링크 추출

 댓글을 크롤링할 기사로 뉴스 랭킹 배너에 조회수 기준 일간 상위 30개의 기사(부문별 5개, 총 6개 부문)을 선정한다.

 네이버 '많이 본 기사' 랭킹 배너 링크를 들어가보면,
("https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20180212")(2018.2.12일자)
일간 누적 집계된 조회수 기준 상위 30개의 6개 부문 기사 제목과 링크가 나열되어 있는 것을 확인할 수 있다.
우리는 여기서 구글 스프레드 시트의 importxml() 함수와 xpath 정규 표현식을 활용하여 일자별 30개의 기사 링크를 긁어올 수 있다.
여기서 랭킹 배너의 링크에는 일정한 규칙성이 있으므로, 랭킹 배너에 접근할 URL은 main_url("https://news.naver.com")에 sub_url 변수를 for문을 통해 조합하는 방식으로 마련하였다.
조사 대상 날짜인 2018.02.06 ~ 2020.06. ** 에서 일자별 자동 추출된 30개의 기사링크는 구글 스프레드시트에 쌓고 "naver_df.xlsx" 파일로 추출한 후 이를 R로 불러온다.

+) 위와 같이 기사 링크를 추출하는 방식은 importxml() 함수를 사용하기 위해 구글 스프레드시트를 거쳐야 하는 불편함이 다소 있었다. 따라서 '다음' 기사 링크를 추출할 때는 Rstudio 내에서 해당 크롤링 작업을 할 수 있도록 하였다. 즉, 'xml2' 패키지의 read_html() 함수와 'rvest' 패키지의 html_nodes() 함수 등을 이용하여 랭킹 배너의 html 소스코드에 열거된 상위 10개 기사의 링크를 가져오도록 했다. (하단 코드 참조)

## 댓글 필터링 및 크롤링, 좋아요/싫어요 수 합산

 크롤링할 댓글 양이 상당하다는 점을 감안해 이미 마련된 패키지를 활용하기로 한다. 사용할 패키지는 N2H4, DNH4이며,
각각 네이버/다음 내부 기사 URL을 인자로 넣으면 해당 기사에 달린 댓글과 좋아요/싫어요 수 등 다양한 변수를 크롤링해준다. 현재까지 문재앙 지수를 산출하는 데 필요한 변수들은 다음과 같다.

  1) 일간 상위 30개 기사에 달린 모든 댓글의 좋아요/싫어요 총합
  2) '문재앙' 및 관련 어휘가 포함/제외된 모든 댓글의 좋아요/싫어요 총합
  3) '문재앙' 및 관련 어휘가 포함/제외된 모든 댓글 수

 그런데 이때, 우리는 타당한 지수 산출을 위해 매크로 및 유령 계정 활용, '좌표 찍기'를 비롯한 댓글상의 정치 공작 등의 불필요한 과도표집을 예방해야 할 것이다. 따라서 특정 댓글 내용을 아예 똑같이 복사한 경우인 기사 내/(동일 일자) 기사 간 중복댓글을 표본에서 배제하고, 중복댓글 중 시간상 마지막에 달린 단 1개의 댓글만을 표본에 반영할 것이다. 또한 '문재앙' 어휘를 포함한 임의의 1000개 댓글을 훑어본 결과,  100개당 1-2개꼴로 현 대통령을 지지하는 댓글을 발견했는데, 대체로 '문재앙 타령' 이라고 말하는 식이었다. 따라서 '문재앙'이라는 어휘가 포함된 댓글 중 '타령'이라는 어휘가 없거나, '탄핵'이라는 강한 부정어를 포함하는 댓글을 수집하는 것으로 2차 필터링을 거쳤다.

```{r crawling_naver, echo = TRUE, eval = FALSE}

main_url = "https://news.naver.com"
operator = "/main/ranking/popularDay.nhn?rankingType=popular_day&date="
datelist = read_xlsx("datelist.xlsx", range = cell_cols("B")) %>% as_tibble()

Summary_3 = tibble()
for (j in 1:nrow(datelist)){
  Summary_1 = tibble()
  Summary_2 = tibble()
  tmp_1 = tibble()
  tmp_2 = tibble()
  date = datelist[j,1]
  doc = read_html(str_c(main_url, operator, date))
  tmp = html_nodes(doc, 'dl') %>% html_nodes('a') %>% html_attr("href")
  tic()
  for (i in 1:30){
    sub_url = tmp[i]
    tmp_1 = getAllComment(str_c(main_url, sub_url)) %>%
      as_tibble() %>%
      filter(!duplicated(contents)) %>%
      select(contents, ends_with("Count"), -imageCount)
    Summary_1 = bind_rows(Summary_1, tmp_1)
    
    tmp_2 = tmp_1 %>%
      filter(str_detect(contents, "문재앙")) %>%
      filter(!str_detect(contents, "타령")) %>%
      select(contents, ends_with("Count"))
    Summary_2 = bind_rows(Summary_2, tmp_2)}
  toc()
  
  nrow = nrow(Summary_1)
  nrow_1 = nrow(Summary_2)
  Summary_1 = Summary_1 %>%
    filter(!duplicated(contents))
  like = sum(Summary_1$sympathyCount)
  dislike = sum(Summary_1$antipathyCount)
  
  Summary_2 = Summary_2 %>%
    filter(!duplicated(contents))
  like_1 = sum(Summary_2$sympathyCount)
  dislike_1 = sum(Summary_2$antipathyCount)

  Summary_3[j, 1] = ymd(date)
  Summary_3[j, 2] = like
  Summary_3[j, 3] = dislike
  Summary_3[j, 4] = nrow
  Summary_3[j, 5] = like_1
  Summary_3[j, 6] = dislike_1
  Summary_3[j, 7] = nrow_1

  print(ymd(date))}

write.csv(Summary_3, "Summary_df_5.csv")

Summary_3$week = as.Date(cut(Summary_3$...1, breaks = "week", start.on.monday = FALSE)) - 1
Summary_3
for (i in 2:767){
  Summary_3$week[i-1] = Summary_3$week[i]}
```

## 다음 기사 링크 추출

 '다음'의 경우 랭킹 뉴스 배너 조회수 기준 일자별 상위 10개 기사를 댓글 추출 대상으로 한다. ('2019 언론수용자 조사'의 포털별 뉴스 점유율에 따라(네이버 91%, 다음 20%, 중복응답) 기사 갯수를 선정했다.) 크롤링 코드는 다음과 같다. 앞서 언급한대로 'xml2', 'rvest' 패키지 내장 함수를 통해 html 소스코드에서 일자별 기사 링크를 추출하고 해당 링크 기사에 달린 댓글들에 접근하는 방식을 취하였다. 이떄 사용한 'DNH4' 패키지의 getAllComment() 함수는 패키지상의 오류가 발생하곤 했으므로 상응하는 예외처리를 거쳤다.
 
```{r crawling_daum, echo = TRUE, eval = FALSE}
library(DNH4)

main_url = "https://media.daum.net/ranking/popular"
operator = "?regDate="
datelist = read_xlsx("datelist.xlsx", range = cell_cols("B")) %>% as_tibble()
linklist = list()

Summary_4 = tibble()
print(datelist, n = 300)

for (i in 1:nrow(datelist)){
  Summary_1 = tibble()
  Summary_2 = tibble()
  date = datelist[i,1] 

  doc = read_html(str_c(main_url, operator, date))
  tmp = html_nodes(doc, 'li') %>% html_nodes('a') %>% html_attr("href")
  linklist = unique(tmp[44:63])

for (j in 1:length(linklist)){
  tic()
  tmp_1 = tibble()
  tmp_2 = tibble()
    sub_url = linklist[j]
    if(is_null(tryNULL(getAllComment(sub_url)))){next} else {
    tmp_1 = getAllComment(sub_url) %>% as.data.table() %>%
      as_tibble() %>%
      filter(!duplicated(content)) %>%
      select(content, likeCount, dislikeCount)
    Summary_1 = bind_rows(Summary_1, tmp_1)
    
    tmp_2 = tmp_1 %>%
      filter(str_detect(content, "문재앙")) %>%
      filter(!str_detect(content, "타령")) %>%
      select(content, likeCount, dislikeCount)
    Summary_2 = bind_rows(Summary_2, tmp_2)}}
  toc()
  
  Summary_1 = Summary_1 %>%
    filter(!duplicated(content))
  nrow = nrow(Summary_1)
  like = sum(Summary_1$likeCount)
  dislike = sum(Summary_1$dislikeCount)
  
  Summary_2 = Summary_2 %>%
    filter(!duplicated(content))
  nrow_1 = nrow(Summary_2)
  like_1 = sum(Summary_2$likeCount)
  dislike_1 = sum(Summary_2$dislikeCount)
  
  Summary_4[i-1, 1] = date
  Summary_4[i-1, 2] = like
  Summary_4[i-1, 3] = dislike
  Summary_4[i-1, 4] = nrow
  Summary_4[i-1, 5] = like_1
  Summary_4[i-1, 6] = dislike_1
  Summary_4[i-1, 7] = nrow_1
  print(i)}
```
```{r Summary_4, include=TRUE}
daum_disaster_reply = read_xlsx("daum_disaster_reply.xlsx")
```
```{r Summary_4_1, echo = TRUE, eval= TRUE}
daum_disaster_reply
```
 
 '문재앙' 어휘를 포함한 댓글이 수집된 데이터를 살펴보자. (예외처리를 거친) 대략 845일간의 '문재앙' 댓글 전량을 수집하였음에도 4500개 댓글뿐으로, 이는 하루 5개 정도의 댓글만이 '문재앙' 어휘를 사용했음을 뜻한다. 다음 플랫폼 주 이용자의 정치적 성향을 고려하더라도 이는 상당히 적은 양의 댓글 수이므로 독립적인 표본으로서 결과에 반영하기 애매한듯 보이므로 우선은 분석 대상에서 제외하는 것이 나을 것 같다.

## 문재앙 지수 만들기 (2) 국정 지지도와 초벌 상관 분석

 이제 수집된 네이버 댓글 데이터와 위키백과의 '대통령 지지도' 항목에 제시된 국정평가 도표 수치를 비교해보자. 대강의 상관관계를 파악하는 데 도움을 줄 것이다.
 
```{r, include= FALSE}
Summary = read_xlsx("full.xlsx")
Summary = Summary[1:120,]
ideo = read_xlsx("poll_president_ideology.xlsx")
daily = read_xlsx("poll_president_daily.xlsx")
disaster.df = read_xlsx("Summary_df_20180206_20200529.xlsx") 
others.df = read_xlsx("others_190904_0529_but575.xlsx")
Summary_3 = tibble()
Summary_3 = Summary
``` 

```{r, echo = TRUE}
cor(Summary_3[1:119,1:6], wiki[,c("gallup", "rlmeter")])
```

다소 뜻밖에 결과를 알 수 있는데, '문재앙' 어휘 포함 댓글의 수, 좋아요/싫어요 수치보다 전체 댓글 수, 좋아요, 싫어요 수가 다소 높은 상관을 보이는 것으로 드러나기 때문이다. 따라서 크롤링한 요인들을 활용하여 산출식을 정교화해야 높은 상관관계를 얻을 수 있겠다고 생각된다.
이쯤에서 조사기간(2018.2 ~ 2020.05) 내 전체 댓글 추이를 한번 살펴보자.

```{r ggplot,echo=TRUE}
ggplot(Summary, aes(x = week, y = num_all)) +
  geom_line() +
  xlab("조사기간\n(2018년 2월 첫째주 ~ 2020년 5월 셋째주)")+
  ylab("네이버 뉴스 전체 댓글 수")+
  scale_y_continuous(breaks = 100000*c(2:5), labels = c("200000개", "300000개", "400000개", "500000개"))+
  theme_bw()+
  geom_smooth(method = "loess")
```

 loess 추세선과 함께 살펴보면, 네이버 뉴스창의 (랭킹 배너 일간 조회수 top30 기사 한정) 댓글 수는 꾸준히 감소해 온 것을 알 수 있다. 다만 올해 들어 코로나 사태 영향으로 댓글창이 일시적으로 활발해진 추세도 함께 엿볼 수 있다. 이렇듯 꾸준한 감소세는 앞서 살펴 본 위키백과 항목의 대통령 지지율의 꾸준한 감소와 유사한 패턴을 보이지만, 이 두 가지 현상, 즉 현 대통령의 레임덕과 (이유 모를) 네이버 댓글창의 비활성화가 유의미한 상관관계를 갖는 사회적 현상인지는 의문이다.  
 
## 문재앙 지수 만들기 (3) 인사이트 파악 후 산출식 수정

여기서 나아가 다른 어휘들은 어떤 상관관계가 있을지까지 파악해 보기로 했다. 코드는 다음과 같다.
(네이버 서버상의 문제인지 같은 일자에서 계속해서 오류가 발생했다. 따라서 for문을 돌리는 와중에 중간중간 오류 발생 시 예외처리를 하고 코드 실행을 재개해야 한다.)

```{r cor.test.2, echo = TRUE, eval = FALSE}
main_url = "https://news.naver.com"
operator = "/main/ranking/popularDay.nhn?rankingType=popular_day&date="
datelist = read_xlsx("datelist.xlsx", range = cell_cols("B")) %>% as_tibble()
# datelist = datelist[774:844, 1]

Summary_3 = tibble()

for (j in 1:nrow(datelist)){
  Summary_2 = tibble()
  Summary_4 = tibble()
  Summary_5 = tibble()
  Summary_6 = tibble()
  date = datelist[j,1]
  doc = read_html(str_c(main_url, operator, date))
  tmp = html_nodes(doc, 'dl') %>% html_nodes('a') %>% html_attr("href")
  tic()
  tmp_1 = tibble()
  tmp_2 = tibble()
  tmp_3 = tibble()
  tmp_4 = tibble()
  tmp_5 = tibble()
  for (i in 1:30){
    sub_url = tmp[i]
    tmp_1 = getAllComment(str_c(main_url, sub_url)) %>%
      as_tibble() %>%
      filter(!duplicated(contents)) %>%
      select(contents, ends_with("Count"), -imageCount)
    
    tmp_2 = tmp_1 %>%
      filter(str_detect(contents, "문죄인")) %>%
      filter(!str_detect(contents, "일베")) %>%
      filter(!str_detect(contents, "타령")) %>%
      select(contents, ends_with("Count"))
    Summary_2 = bind_rows(Summary_2, tmp_2)
    
    tmp_3 = tmp_1 %>%
      filter(str_detect(contents, "중국몽")) %>%
      filter(!str_detect(contents, "일베")) %>%
      filter(!str_detect(contents, "타령")) %>%
      select(contents, ends_with("Count"))
    Summary_4 = bind_rows(Summary_4, tmp_3)
    
    tmp_4 = tmp_1 %>%
      filter(str_detect(contents, "문슬람")) %>%
      filter(!str_detect(contents, "일베")) %>%
      filter(!str_detect(contents, "타령")) %>%
      select(contents, ends_with("Count"))
    Summary_5 = bind_rows(Summary_5, tmp_4)
    
    tmp_5 = tmp_1 %>%
      filter(str_detect(contents, "대깨문")) %>%
      filter(!str_detect(contents, "일베")) %>%
      filter(!str_detect(contents, "타령")) %>%
      select(contents, ends_with("Count"))
    Summary_6 = bind_rows(Summary_6, tmp_5)}
  toc()
```
```{r, include=FALSE, eval=FALSE}
  
  Summary_2 = Summary_2 %>%
    filter(!duplicated(contents))
  Summary_4 = Summary_4 %>%
    filter(!duplicated(contents))
  Summary_5 = Summary_5 %>%
    filter(!duplicated(contents))
  Summary_6 = Summary_6 %>%
    filter(!duplicated(contents))
  
  nrow_1 = nrow(Summary_2)
  nrow_2 = nrow(Summary_4)
  nrow_3 = nrow(Summary_5)
  nrow_4 = nrow(Summary_6)

  like_1 = sum(Summary_2$sympathyCount)
  dislike_1 = sum(Summary_2$antipathyCount)
  like_2 = sum(Summary_4$sympathyCount)
  dislike_2 = sum(Summary_4$antipathyCount)
  like_3 = sum(Summary_5$sympathyCount)
  dislike_3 = sum(Summary_5$antipathyCount)
  like_4 = sum(Summary_6$sympathyCount)
  dislike_4 = sum(Summary_6$antipathyCount)
  
  Summary_3[j, 1] = ymd(date)
  Summary_3[j, 2] = like_1
  Summary_3[j, 3] = dislike_1
  Summary_3[j, 4] = nrow_1
  Summary_3[j, 5] = like_2
  Summary_3[j, 6] = dislike_2
  Summary_3[j, 7] = nrow_2
  Summary_3[j, 8] = like_3
  Summary_3[j, 9] = dislike_3
  Summary_3[j, 10] = nrow_3
  Summary_3[j, 11] = like_4
  Summary_3[j, 12] = dislike_4
  Summary_3[j, 13] = nrow_4
        
  print(ymd(date))}

write.csv(Summary_3, "others_190904_0529_but575.csv")

Summary_3$week = as.Date(cut(as.Date(Summary_3$...7), breaks = "week", start.on.monday = FALSE)) - 1
for (i in 2:838){
  Summary_3$week[i-1] = Summary_3$week[i]}

Summary_full = tibble()
for (i in 1:121){
  Summary_full[i,1] = sum(Summary_3$...2[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,2] = sum(Summary_3$...3[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,3] = sum(Summary_3$...4[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,4] = sum(Summary_3$...5[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,5] = sum(Summary_3$...6[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,6] = sum(Summary_3$...7[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,7] = sum(Summary_3$...8[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,8] = sum(Summary_3$...9[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,9] = sum(Summary_3$...10[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,10] = sum(Summary_3$...11[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,11] = sum(Summary_3$...12[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,12] = sum(Summary_3$...13[Summary_3$week == unique(Summary_3$week)[i]])
  Summary_full[i,13] = unique(Summary_3$week)[i]}

write_xlsx(Summary_full, "full.xlsx")
```

(엑셀 스프레드시트로 변수명 변경 및 열 배치 등 간단한 전처리를 했다.)

나아가, 보다 구체적이고 유의미한 결과 도출을 위해 나는 주 단위 단순 긍정/부정평가 항목 이외에 '정치성향별 국정평가' 항목 및 일 단위 긍정/부정평가 항목 수치까지 수집했다. 방식은 역시 주간집계 PDF 보고서 파일에서 스프레드시트로 일일이 복사+붙여넣기하는 노가다에 가까웠는데, 이 과정을 효율적으로 운용하기 위해서는 여론조사 기관의 간결한 결과 제시가 필요해 보인다.

자, 이제 우리는 다양한 산출식을 생각해 볼 수 있다. 나는 우선 3가지 산출식 옵션을 생각했는데, <strong>3가지 옵션</strong>은 다음과 같다.

#### 1. 전체 좋아요 수 / (전체 좋아요 + 싫어요 수) - 문재앙 좋아요 수 / (문재앙 좋아요 + 싫어요 수)
#### 2. 좋아요 수 / (좋아요 + 싫어요 수)
#### 3. {(좋아요 + 싫어요 수) / 좋아요 수}*log(단어 포함 댓글 수/전체 댓글 수)

한눈에 보기에도 가장 간단한 식은 2번이다. 비하 단어를 포함한 댓글의 주간 좋아요 수와 싫어요 수 합산만 고려하면 된다. 여기에 주간 댓글 좋아요/싫어요 동향을 반영하여 두 수치를 빼면 1번 식이 나오고, 주간 댓글 좋아요/싫어요 동향을 반영하는 대신, 주간 전체 댓글 수 대비 비하 단어 포함 작성된 댓글 수 비율의 가중치를 두면 3번 식이 나온다. 이때 비하 단어 포함 댓글은 전체 댓글에 비해 상당히 적은 비율을 갖고 있기 때문에 이를 고려하여 로그 변환을 해주는 것이 나을 것이라 판단했다.

댓글 및 좋아요/싫어요 수가 상식적으로 나온다면, 1번과 3번 식은 국정 지지도와 정적 상관을, 2번 식은 부적 상관을 가지게 될 것이다. 정말로 그러한지 확인해볼겸 나는 1차적인 수식 검증을 할 것이다. 5개의 비하 단어와 32개의 기본 국정평가 항목 중 각각이 얼마큼 유의미한 상관관계를 갖는지 확인한다. 160개의 항목 중 유의도 .05를 넘는 항목의 갯수가 크게 낮으면 정확한 지수 산출식이라고 볼 수 없을 것이다.

#### 예시
##### Option 1. 
###### 전체 좋아요 수 / (전체 좋아요 + 싫어요 수) - 문재앙 좋아요 수 / (문재앙 좋아요 + 싫어요 수)
```{r, eval=TRUE, echo=TRUE}

Summary.df = Summary %>%
  mutate(disaster = like_all/(like_all+dislike_all) - (like_1)/(like_1+dislike_1),
         sinner = like_all/(like_all+dislike_all) - (like_2)/(like_2+dislike_2),
         china = like_all/(like_all+dislike_all) - (like_3)/(like_3+dislike_3),
         islam = like_all/(like_all+dislike_all) - (like_4)/(like_4+dislike_4),
         head = like_all/(like_all+dislike_all) - (like_5)/(like_5+dislike_5)) %>%
  select(week, disaster, sinner, china, islam, head, everything())

Summary.df = full_join(Summary.df, rlmeter_president, by = "week") %>% 
  full_join(gallup_president, by = "week") %>%
  select(-week) %>%
  bind_cols(ideo) %>% 
  select(-week)

mySumm = tibble()
for (i in colnames(Summary.df[1:5])){
  for (j in colnames(Summary.df[24:55])){
    formula = as.formula(str_c("~",i,"+",j))
   myCorr = cor.test(formula, data = Summary.df) %>% tidy()
   myCorr$word = i
   myCorr$eval = j
    mySumm = bind_rows(myCorr, mySumm)
  }
}
mySumm %>% arrange(p.value) %>%
  select(word, eval, estimate, everything(), -method) %>%
  filter(p.value<.05)
```
```{r, include= FALSE}
Summary.df = Summary.df %>%
  mutate(disaster = (like_1)/(like_1+dislike_1),
         sinner = (like_2)/(like_2+dislike_2),
         china = (like_3)/(like_3+dislike_3),
         islam = (like_4)/(like_4+dislike_4),
         head = (like_5)/(like_5+dislike_5)) %>%
  select(disaster, sinner, china, islam, head, everything())


mySumm = tibble()
for (i in colnames(Summary.df[1:5])){
  for (j in colnames(Summary.df[24:55])){
    formula = as.formula(str_c("~",i,"+",j))
    myCorr = cor.test(formula, data = Summary.df) %>% tidy()
    myCorr$word = i
    myCorr$eval = j
    mySumm = bind_rows(myCorr, mySumm)
  }
}

Summary.df.2 = Summary.df %>%
  mutate(disaster = {(like_1+dislike_1)/(like_1)}*log(num_1/num_all),
         sinner = {(like_2+dislike_2)/(like_2)}*log(num_2/num_all),
         china = {(like_3+dislike_3)/(like_3)}*log(num_3/num_all),
         islam = {(like_4+dislike_4)/(like_4)}*log(num_4/num_all),
         head = {(like_5+dislike_5)/(like_5)}*log(num_5/num_all)) %>%
  select(disaster, sinner, china, islam, head, everything())

# Summary.df.2 = bind_cols(Summary.df.2, ideo) %>% select(-week)

mySumm.2 = tibble()
for (i in colnames(Summary.df.2[1:5])){
  for (j in colnames(Summary.df.2[24:55])){
    formula = as.formula(str_c("~",i,"+",j))
    myCorr = cor.test(formula, data = Summary.df.2) %>% tidy()
    myCorr$word = i
    myCorr$eval = j
    mySumm.2 = bind_rows(myCorr, mySumm.2)
  }
}
```
```{r, echo=TRUE}
# option 2
mySumm %>% arrange(p.value) %>%
  select(word, eval, estimate, everything(), -method) %>%
  filter(p.value<.05)
```
```{r, echo=TRUE}
# option 3
mySumm.2 %>% arrange(p.value) %>%
  select(word, eval, estimate, everything(), -method) %>%
  filter(p.value<.05)
```

위와 같은 코드로 3개의 산출식을 계산했을 때 각각 <strong>106, 108, 121개</strong>의 유의미한 항목 결과가 나온다. 모름/무응답 항목도 포함된 만큼 세 가지 산출식 모두 다소 유의한 항목 갯수라고 볼 수 있을 듯하다. 2번 산출식에서 전체 댓글 좋아요 주간 동향을 반영한 1번 산출식이 조금 더 유의도가 떨어졌다는 점에서 '비하 단어' 포함 댓글은 전체 댓글의 좋아요 수를 고려하지 않았을 때 더욱 여론의 효과적인 재현 지표로 볼 수 있다는 것을 알 수 있다. 또한 3번 산출식이 정확도가 가장 높아보이는데, 2번과 3번의 scattorplot을 보자.

```{r, echo=TRUE, eval=TRUE}
## Option 2
par(mfrow = c(2, 2))
plot(Summary.df$disaster, Summary.df.2$잘못한다, main = "문재앙 지수")
plot(Summary.df$sinner, Summary.df.2$잘못한다, main = "문죄인 지수")
plot(Summary.df$islam, Summary.df.2$잘못한다, main = "문슬람 지수")
plot(Summary.df$head, Summary.df.2$잘못한다, main = "대깨문 지수")
```
```{r, echo=TRUE, eval=TRUE}
## Option 3
par(mfrow = c(2, 2))
plot(Summary.df.2$disaster, Summary.df.2$잘못한다, main = "문재앙 지수")
plot(Summary.df.2$sinner, Summary.df.2$잘못한다, main = "문죄인 지수")
plot(Summary.df.2$islam, Summary.df.2$잘못한다, main = "문슬람 지수")
plot(Summary.df.2$head, Summary.df.2$잘못한다, main = "대깨문 지수")
```

어떤 지수를 활용할 지는 우리 마음이지만, 나는 가장 단순한 2번 산출식과 estimate가 가장 높은 항목을 도출한 3번 산출식 이 두 가지 지표를 모두 활용해서 각각 시각화해보기로 했다. 본격적인 시각화와 상응하는 주관적 분석에 대해서는 다음 게시물에서 다루도록 하겠다.

<mark>(2편에 계속)</mark>

<style>
body {
  font-family: NanumGothic;
  fontsize = 8px
}
</style>